\chapter{Interfacing With The Kinect Sensor}
\ifpdf
    \graphicspath{{Chapter3/Chapter3Figs/PNG/}{Chapter3/Chapter3Figs/PDF/}{Chapter3/Chapter3Figs/}}
\else
    \graphicspath{{Chapter3/Chapter3Figs/EPS/}{Chapter3/Chapter3Figs/}}
\fi



The Kinect SDK provides access to the Kinect Sensor from within C\# code by defining a \verb|KinectSensor| object which interfaces directly with the hardware. When this object is instantiated, and provided a Kinect Sensor is connected via the USB port, it provides access to the depth, RGB and skeletal streams of the camera. The \verb|KinectSensor| object also provides an event  \verb|AllFramesReady| which fires each time the streams have successfully updated. The streams update at approximately 30 frames per second but this rate can be significantly reduced if the sensor attempts to track too many skeletons. Using this \verb|KinectSensor| object we built a general purpose skeletal tracking controller class \verb|GestureController| (Figure:~\ref{fig:umlgest}) which initialises a \verb|KinectSensor| object to provide a single skeletal stream and subscribes to the \verb|AllFramesReady| event via a method \verb|KinectAllFramesReady|.

\section{Recording Training Data}
In order to record the training data we implemented a class \verb|GestureRecorder|, which extends \verb|GestureController|, and a class \verb|GestureRecording| which stores a single recording of the skeleton over time (Figure:~\ref{fig:umlgest}). This second class contains a hashmap from \verb|JointType| (an enum listing the different possible joints tracked by the Kinect Sensor) to lists of vectors representing the readings of that joint through time, and a method \verb|addReading| which takes a \verb|Skeleton| object and adds each joint location to the appropriate list. The \verb|GestureRecorder| class stores a list of completed \verb|GestureRecording| objects along with a current recording and extends the \verb|KinectAllFramesReady| method to pass the Skeleton provided by the sensor to this current recording. This class also provides methods to start and stop the current recording (adding it to the list of recordings when stopped) and to save the list of recordings to file. Thus the \verb|GestureRecorder| class allows us to record the movements of the hands, wrists, elbows, shoulders and head over time and save them as training or test data. Further we implemented two methods for storing the data -- as positions relative to the camera or as positions releative to the signer's head.

\begin{figure}[t]
        \centering
        \includegraphics[width = 1.05\textwidth]{ThesisFigs/gestRecDiag}
        \caption{A UML diagram of the gesture recording classes}\label{fig:umlgest}
\end{figure}

Using this controller we implemented a small program which can be used to record a collection of training data by repeating a sign the desired number of times and giving a start/stop signal between each. For convenience we defined this signal to be when hands are raised a certain distance above the waist, a signal consistent with the pose a signer takes whilst pausing between sequences of signs.
 
\subsection{The Training Data}
Using this controller we recorded for each of a collection of 20 British Sign Language (see figure:~\ref{fig:signfig}) signs a set of 40 training examples, 10 hold-out examples and 10 testing examples. Each sign lasted between 1 and 3 seconds and was recorded at the maximum skeletal frame rate -- 30 frames per second.

\begin{figure}[t]
        \centering
        \includegraphics[width = 1.00\textwidth]{ThesisFigs/signFig}
        \caption{The collection of 20 signs we recorded training data for (image credit: www.british-sign.co.uk)}\label{fig:signfig}
\end{figure}

\subsection{Limitations in the Training Data}
The quality of the training sets was degraded by two factors. The first is that the signer (myself) who performed the training sets is not fully fluent in sign language and hence does not reproduce the signs consistently. The second factor is a limitation of the Kinect Sensor. The Kinect can track joints well when they are isolated but performs poorly when two parts of the body, for example the hands, are in contact (see figure:~\ref{fig:brokht}) and as many common signs require the hands to be placed together this issue caused a significant amount of the noise in the training data for those signs. The hand tracking issue is expected to be fixed in an upcoming SDK release but the contact issue could still cause noise in certain signs; for example those that require the signer to touch their elbows, shoulders or face.

\begin{figure}[h!]
        \centering
        \includegraphics[width = 0.8\textwidth]{ThesisFigs/brokenHandTracking}
        \caption{An example of the broken hand tracking with the hands placed together. The yellow joints are inferred by the sensor incorrectly, the actual location of the hands is just below the chin.}\label{fig:brokht}
\end{figure}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
