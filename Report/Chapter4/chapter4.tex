\chapter{Testing and Experimentation}
\ifpdf
    \graphicspath{{Chapter3/Chapter3Figs/PNG/}{Chapter3/Chapter3Figs/PDF/}{Chapter3/Chapter3Figs/}}
\else
    \graphicspath{{Chapter3/Chapter3Figs/EPS/}{Chapter3/Chapter3Figs/}}
\fi

\section{Determining the Classifier Parameters}
In building the \verb|signClassifier| we introduced parameters to determine how the sign likelihoods are calculated. For example in the \verb|signModel| class we used k-means clustering to quantize the continuous Kinect sensor reading and hence had to chose a value for $k$ (and hence the number of observation symbols $M$). Further, in the \verb|signClassifier| we introduced a threshold value $\tau$ which determined whether the likelihood that a sign was observed was high enough to conclude that it indeed was observed. The choices of these parameter values have a significant impact on the performance of our classifier and for this reason we determined their values by experimentation. To avoid overfitting our model by training on the test data set we instead used a hold-out set for these experiments.

In carrying out our experiments we used several standard measures to quantify the performance of our classifier, for example measures of \emph{true positives}, \emph{true negatives}, \emph{false positives} and \emph{false negatives}. We take the definition of negative here to mean that no sign was/should be detected. Hence if the input for one sign is erroneously classified as a different sign this does not count as a false positive but rather as a \emph{misclassification}. The definitions of these terms are summarised in table \ref{tab:errors}.
\begin{table}[h!]
  \centering
  \begin{tabular}{c c|c c c c c c}         
\multicolumn{7}{c}{Hypothesised Sign} \\   
    			& & 1 & 2 & 3 & \dots & 20 & No Sign \\
  \hline
\multirow{6}{*}{\begin{sideways} Actual Sign \end{sideways}} &  1 & TP & MC & MC & \dots& MC&FN \\
  &2 		 	& MC & TP & MC & \dots& MC&FN \\
  &3 			& MC & MC & TP & \dots& MC&FN \\
  &\vdots 	& \vdots & \vdots & \vdots& $\ddots$ & \vdots & \vdots \\
  &20 		& MC & MC & MC &  \dots & TP & FN \\
  &No Sign 	& FP & FP & FP & \dots & FP & TN \\

  \end{tabular}
  \caption{A confusion matrix showing when different performance measures occur in a classifier trained to recognise 20 signs. Key: MC - misclassification, TP - true positive, TN - true negative, FP - false positive, FN - false negative. }
  \label{tab:errors}
\end{table}

\subsection{The Cluster Number}
As each choice of cluster number, $k$, defines a different classifier we decided to test these different classifiers to find a value for $k$ which optimises the perfomance of our system. There are a number of methods to measure how well a given classifier performs in comparison to others. One technique is to plot ROC curves - plots of true-positive rate against the false-positive rate - of the different classifiers and use these to determine the best performance (either by observation or calculation of the area under the curve). This method is well documented for binary classifiers and has been extended for use with multiple bin classifiers, for example by considering higher dimensional ROC curves~\citep{landgrebe2006simplified}. As we tested our system on a dictionary of $20$ signs, this method would require us to consider a hypersurface in at least 20 dimensions and would require a very large hold-out data set to provide meaningful insight into the performance of our classifier. Due to the time restrictions of this project it was not possible to collect sufficiently large data sets and hence this method for comparing classifiers was unsuitable. 

A second method for comparing classifiers is to consider a one class vs all others binary classifier for each class and to generate a 2-dimensional ROC curve for each. We attempted to use this method to determine the ideal cluster number but quickly found it to be innappropriate as for a given sign the number of false positives (that is test data streams that are incorrectly classified as that sign) was extremely low; regardless of the thresholding value and cluster number. As a result the ROC plots were mostly degenerate, providing only information about the true positive rate, and could not be use to determine an appropriate number for $k$.

As such we decided to use the accuracy, defined to be 
\begin{equation*}
\text{accuracy} = \frac{\text{true positives}+\text{true negatives}}{\text{number of tests}}
\end{equation*}
as our measure of performance for each classifier. For cluster numbers ranging from $3$ to $8$ we retrained the classifier, tested it on the hold-out set for different threshold values and recorded its accuracy measure (see figure~\ref{fig:clustTest}). This experiment provided some unexpected results, showing that a cluster number of $4$ made for a more accurate classifer. We had assumed that a higher cluster number would make for a more accurate classifier because a more fine grained discretisation of the continuous input stream should allow the classifier to utilise more information about the input stream. However this was not the case and we found that the smaller cluster number of $k=4$ yielded an accuracy of 80.9\% (for $\tau = -800$) which was considerably (almost 20\%) better than classifiers trained with a higher cluster number; as such we chose a cluster number of $k=4$. Further inspection of the parameterisations of the \verb|signModel| objects with higher cluster number found that many clusters contained few or no points and this may have caused the translation from continuous space to observation symbol space to translate very similar continous streams to considerably different symbol streams.

\begin{figure}[]
        \centering
        \includegraphics[width=1.0\textwidth]{ThesisFigs/clusterNumberTest}
        \caption{Recognition accuracy of classifiers with different cluster numbers}\label{fig:clustTest}
\end{figure}


\subsection{The Acceptance Threshold}
Given a collection of joint observations $\bm{\Theta}$, the sign classifier determines the sign model $m$ which is most likely to have generated it and if $L[\mathcal{\Theta}|m]$ is larger than some threshold $\tau$, concludes that $\bm{\Theta}$ corresponds to the sign that trained $m$. However there is no \emph{a priori} reason to choose a given value for probability threshold $\tau$. If $\tau$ is chosen too small then the classifier is more likely to associate a non-sign observation sequence with a sign, resulting in a false positive. Conversely, if $\tau$ is chosen too great then the classifier may fail to associate the observation sequence of a sign with the appropriate sign model, resulting in a false negative. To determine the appropriate value of $\tau$ we trained the classifier on the half of the training training set and tested the classifier on the full hold-out data set for values of $\tau$ from $-2000$ up to $0$ in increments of $50$. In training our classifier on only half of the training data we essentially designated the signs from the other half as signs that should be classified as ``No Sign'' by our classifier and hence provided a large enough data set to test for false positives. The rationale here is that recording a large test data set of nonsense gestures is particularly time consuming and adds little value beyond this specific test. We recorded for each value of $\tau$ the number of false postives and false negatives and plotted them together (see figure:~\ref{fig:tradeOff}), from this experiment and by considering the accuracy measure we determined $\tau = -300$ to be an appropriate choice. 

\begin{figure}[t]
        \centering
        \includegraphics[width=0.9\textwidth]{ThesisFigs/thresholdTradeoff}
        \caption{The trade off between false positives and false negatives}\label{fig:tradeOff}
\end{figure}

\section{Lowering the Misclassification Rate}
Whilst false positive and false negatives can be altered by changing the threshold value it is considerably more difficult to prevent misclassifications - the situation where the input sign for some sign is classified as another. From experimentation we find, predictably, that the misclassification rate increases with the number of signs (see figure:~\ref{fig:misclassTest}). It seems then that the only way we can reduce the misclassification rate is to reduce the number of signs in our system is expected to distinguish between.

\begin{figure}[t]
        \centering
        \includegraphics[width=0.9\textwidth]{ThesisFigs/misclassTest}
        \caption{A graph comparing the number of signs the classifier decided between to the misclassification rate. As expected if there are more possible signs to decide between the misclassification rate increases.}\label{fig:misclassTest}
\end{figure}

It is in fact possible to reduce the number of signs considered in each classification without reducing the total number of signs the system is able to recognise. To do this we can restrict the grammar of the sentences our classifier will recognise to so that it need only look into a certain subclass of words when attempting to recognise a sign. As BSL uses a topic-comment sentence structure we could restrict to a simple sentence structure, for example \emph{noun-pronoun-question} which encapsulates questions such as ``what is your name?'' (name-your-what). Then when the classifier receives the first input stream it will need only look amongst the sign models for nouns to try and place the sign and will not suffer from misclassifications between nouns and other sign types. Following this the classifier, having just recognised a noun, will need only look amongst the pronoun sign models to classify the next input sequence. 

To test the difference a grammar restriction could make our classifier system we grouped our signs by word class into one of \emph{pronoun, question, noun} or \emph{adjective}. We then retested the classifier on the hold-out set but this time only considering sign models with the same word class as the test case. This method reduced our misclassification rate from 18.7 \% to 12\% and increased the accuracy of our classifier to 84\%.

Using a restricted grammar for our system certainly impacts on its utility and expressiveness and this is undesirable. The results of this experiment do however show us that we can significantly improve our classification accuracy by removing one of our most restrictive assumptions - that signs will be performed independently of one another. By restricting to a specific grammar we have reversed this assumption entirely and insisted that signs can only occur in one of a number of predetermined orders, an assumption that again is deleteriously restrictive but this time impacting the expressiveness of our system rather than the accuracy. It seems the ideal solution lies somewhere between the two approaches, allowing each sign model knowledge of the previously recognised signs so that it can be factored in to the likelihood calculation. This approach requires a great deal of knowledge of the grammar of sign language to be made useful and was hence beyond the scope of this project. It does however suggest an avenue of further research - building a model of the grammar of sign language and using it to predict the most likely word class of the next word in a partial setence.

\section{Data Representation}
The training, hold-out and test data sets were stored in two forms when they were collected. The first from was as absolute positions, that is as points relative to the kinect camera, and the second was as positions relative to the signer's head. At the time we collected these training sets it wasn't clear which data representation would be most effective. For example, the absolute representation generates input streams of vectors which vary more greatly and hence does not amplify minor errors and variations in signs as much as a relative representation whilst the relative representation reduces the signer dependence of the input stream and the effect of the sensor position on the recognition rate. As all of the data was recorded by the same signer and in the same location, it seemed that the advantages of a relative representation would be lost and it was for this reason we decided to focus on experimentation with an absolute representation. However we decided to check our intuition by experimenting with the two representations.

We began by performing each of the experiments above but this time using the relative representation as opposed to the absolute one. Using this method we determined again that $k=4$ is an ideal cluster number and that a threshold value of $\tau = -400$ gives optimal accuracy. After retraining the classifier we tested it on the hold-out set and found it had an accuracy of 85.3\% without a grammar restriction and an accuracy of 95.6\% with the restricted grammar. As the relative representation performs considerably better than the absolute representation it is this representation we chose to use for our classifier. 
% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
