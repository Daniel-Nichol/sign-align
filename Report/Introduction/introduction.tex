%%% Thesis Introduction --------------------------------------------------
\chapter{Introduction}
\ifpdf
    \graphicspath{{Introduction/IntroductionFigs/PNG/}{Introduction/IntroductionFigs/PDF/}{Introduction/IntroductionFigs/}}
\else
    \graphicspath{{Introduction/IntroductionFigs/EPS/}{Introduction/IntroductionFigs/}}
\fi

British Sign Language is the preferred first language of over 125,000 in the United Kingdom, in addition to an estimated 20,000 children and thousands of hearing friends, relatives and interpreters. There are many more people worldwide communicating in an estimated 200 different signed languages, with a huge variation in grammar and pronounciation. As such, a system which can recognise signed languages and convert them into text in real time would be a valuable tool for many people. 

The aim of this project is to implement a system to recognise gestures of a signed language from a data stream provided by the Microsoft Kinect camera. The problem of continuous gesture recognition has been studied for over 20 years and has seen solutions which often involve specific gloves or expensive hardware and which have been particularly sensitive to lighting conditions and camera placement. Using the Kinect sensor, which is available to purchase off the shelf and is designed to be used without gloves and in a range of conditions, we aim to build a robust sign recognition system which does not suffer from these limitations.

\section{Sign Language}
Signed languages are natural languages which have evolved independently of the spoken languages of the areas in which they are used and often independently of one another. For example, British Sign Language (BSL) is not simply oral English transcribed but rather a distinct language with its own sentence structure which differs significantly from English. Further despite the regions sharing a common language, American Sign Language (ASL) is a completely seperate language from BSL.

Despite the large variation between regional sign languages the means of communication remain the same. The main component of the sign is the movement of the body and each sign is determined by the movement and location of the hands and arms as well as the hand shape and palm direction. In addition to the manual component of the signs the signer will often use posture, facial expressions, mouth shape or eye movements to convey meaning.

The non-manual components of sign language will be ignored for the purposes of this project. It should be noted that eye tracking and facial expression are essentially independent problems from that of gesture recognition and hence if solutions to the facial expression problem exist they may be combined with the work of this project to produce a more substantial sign recognition system.

We can further break the manual part of a sign in to two components. The first is the movement of the hand, arms and body over time and the second is the orientation of the hand throughout this movement. This is a sensible distinction to make as the same hand shape might be used with two different arm movements to produce different words and so we can reduce the number of distinct gestures we wish to detect by detecting hand movement gestures and hand shapes and then combining the two to identify a sign.

In the next section we will see there is a limitation in our choice of hardware that makes hand shape detection a problem which is outside the scope of this project. However the above observation suggests that the work in this project to detect arm and body gestures can be combined with future work to produce a system which will detect a signed language. Furthermore, a major task of this project is to implement a library of Hidden Markov Model classifiers to detect gestures and this library can be used to detect hand shape gestures just as we use it here to detect body gestures. 

\section{Hardware}
The Kinect is a motion sensing camera designed by Microsoft and released in November 2010. The Kinect combines an RGB camera and an infrared depth sensor to detect objects in 3D space. The raw images of these sensors are combined using proprietary software to detect a human body as a skeleton, in particular the Kinect is capable of detecting the positions of the hands, elbows, shoulders and head as points in 3D space.

Microsoft released the Kinect software development kit (SDK) for public use on January 16, 2011. This SDK allows developers build applications which interact with the proprietary skeleton tracking software using C\#, C++ or Visual Basic. In this project we will use this SDK to build a gesture recognition framework.

The Kinect camera was originially designed with the ability to detect hand and finger positions. In fact the original patent claims the device would be able to detect American Sign Language ~\citep{LATTA:2010:biblatex}, however this functionality was removed from the original release of the Kinect sensor. The SDK was updated to recognise open and closed hands on March 18th, 2013~\citep{Microsoft:2013:Online} and it is believed that the next iteration of the Kinect hardware will be able to fully detect hand gestures.

Third party hand gesture recognition frameworks do exist however the most successful of these do not make use of the Kinect SDK~\citep{Forth:2013:Online}. We aim to build a framework that will be easily extended when the capabilities of the Kinect are improved and for this reason we choose to only use the tools compatible the Kinect SDK.

\section{Project Outline}
The majority of this project is dedicated to answering the following question: Given a stream of input from the Kinect Sensor and a known sign, what is the probability that the person in the input stream was performing the sign? To  answer this question we implemented a \verb|signModel| class which can be instantiated and trained for each sign and used to determine the probability that an input stream specific sign. These \verb|signModel| objects are then combined into a \verb|signClassifier| object which determines which, if any, sign an input stream corresponds to.

A second component of this project is to implement a means of reading, preparing and storing the skeletal stream provided by the Kinect Sensor to build a set of training and test data for the \verb|signClassifier|. Figure X shows a basic overview of the structure of the signAlign system.

In chapter 2 we introduce the theory of Hidden Markov Models and explore their strengths and limitations with regard to the problem of gesture recognition. In chapter 3 we show how we implemented and combined these models to create the \verb|signModel| objects and how these were then combined to build a \verb|signClassifier|. In chapter 4 we show how we built an interface to the Kinect Sensor and how we collected training, hold-out and testing data sets for the classifier. In chapter 5 we explore how the parameters chosen in building the classifier effect the overall accuracy of the system and use the hold-out set to determine parameter values which optimise performance. Finally in chapters 6 and 7 we evaluate the success of the signAlign system and offer extensions to the system which could be used to improve the accuracy and range of communication that the system can translate.


%%% ----------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
