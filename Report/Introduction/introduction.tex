%%% Thesis Introduction --------------------------------------------------
\chapter{Introduction}
\ifpdf
    \graphicspath{{Introduction/IntroductionFigs/PNG/}{Introduction/IntroductionFigs/PDF/}{Introduction/IntroductionFigs/}}
\else
    \graphicspath{{Introduction/IntroductionFigs/EPS/}{Introduction/IntroductionFigs/}}
\fi

British Sign Language is the preferred first language of over 125,000 adults in the United Kingdom, in addition to an estimated 20,000 children and thousands of hearing friends, relatives and interpreters. There are many more people worldwide communicating in an estimated 200 different signed languages, with a huge variation in grammar and pronunciation. As such, a system which can recognise signed languages and convert them into text in real time would be a valuable tool for many people. 

The aim of this project is to implement a system to recognise gestures of a signed language from a data stream provided by the Microsoft Kinect camera. The problem of continuous gesture recognition has been studied for over 20 years and has seen solutions which often involve specific gloves or expensive hardware and which have been particularly sensitive to lighting conditions and camera placement. Using the Kinect sensor, which is available for purchase off the shelf and is designed to be used without gloves and in a range of conditions, we aim to build a robust sign recognition system which does not suffer from these limitations.

\section{Sign Language}
Signed languages are natural languages which have evolved independently of the spoken languages of the areas in which they are used and often independently of one another. For example, British Sign Language (BSL) is not simply oral English transcribed but rather a distinct language with its own sentence structure which differs significantly from English. Furthermore, despite the regions sharing a common language, American Sign Language (ASL) is a completely separate language from BSL.

Despite the large variation between regional sign languages the means of communication remain the same. The main component of the sign is the movement of the body and each sign is determined by the movement and location of the hands and arms as well as the hand shape and palm direction. In addition to the manual component of the signs the signer will often use posture, facial expressions, mouth shape or eye movements to convey meaning.

The non-manual components of sign language will be ignored for the purposes of this project. It should be noted that eye tracking and facial expression are essentially independent problems from that of gesture recognition and hence if solutions to the facial expression problem exist they may be combined with the work of this project to produce a more substantial sign recognition system.

We can further break the manual part of a sign in to two components. The first is the movement of the hand, arms and body over time and the second is the orientation of the hand throughout this movement. This is a sensible distinction to make as the same hand shape might be used with different arm movements to produce different signs and so we can reduce the number of distinct gestures we wish to detect by detecting hand movement gestures and hand shapes and then combining the two.

In the next section we will see there is a limitation in our choice of hardware that makes hand shape detection a problem, the solution to which lies outside the scope of this project. However the work in this project to detect arm and body gestures can be combined with future work to produce a system which will more accurately detect a signed language. 

\section{Hardware}
The Kinect is a motion sensing camera designed by Microsoft and released in November 2010. The Kinect combines an RGB camera and an infrared depth sensor to detect objects in 3D space. The raw images of these sensors are combined using proprietary software to detect a human body as a skeleton, in particular the Kinect is capable of detecting the positions of the hands, elbows, shoulders and head as points in 3D space.

Microsoft released the Kinect software development kit (SDK) for public use on January 16, 2011. This SDK allows developers build applications which interact with the proprietary skeleton tracking software using C\#, C++ or Visual Basic. In this project we will use this SDK to build a gesture recognition framework.

The Kinect camera was originially designed with the ability to detect hand and finger positions. In fact the original patent claims the device would be able to detect American Sign Language ~\citep{LATTA:2010:biblatex}, however this functionality was removed from the original release of the Kinect sensor. The SDK was updated to recognise open and closed hands on March 18th, 2013~\citep{Microsoft:2013:Online} and it is believed that the next iteration of the Kinect hardware will be able to fully detect hand gestures. Third party hand gesture recognition frameworks do exist, however the most successful of these do not make use of the Kinect SDK~\citep{Forth:2013:Online}. We aim to build a framework that will be easily extended when the capabilities of the Kinect are improved and for this reason we choose to only use the tools compatible the Kinect SDK.

\section{Project Outline}

Figure~\ref{fig:outline} shows a basic outline of the system built during in this project. The SignAlign system takes an input stream from the Kinect sensor, converts it to an appropriate format and uses a trained classifier to determine the sign that was performed to generate the input stream. The majority of this project is dedicated to building and parameterising the sign classifier and implementing a means to collect training data. In chapter 2 we introduce the theory of Hidden Markov Models and explore their strengths and limitations with regard to the problem of gesture recognition. In chapter 3 we show how we implemented and combined these models to create models of signs and how these were then combined to build the classifier. In chapter 4 we show how we built an interface to the Kinect Sensor and collected skeletal stream data. In chapter 5 we explore how the parameters chosen in building the classifier affect the overall accuracy of the system and use a hold-out set to determine parameter values which optimise performance. Finally in chapter 6 we evaluate the success of our system and explore possible extensions to SignAlign.

\begin{figure}[]
  \centering
    \includegraphics[width=1.0\textwidth]{ThesisFigs/overview}
  \caption{A basic outline of the SignAlign system}\label{fig:outline}
\end{figure}

%%% ----------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
