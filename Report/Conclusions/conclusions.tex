\def\baselinestretch{1}
\chapter{Analysis and Conclusions}
\ifpdf
    \graphicspath{{Conclusions/ConclusionsFigs/PNG/}{Conclusions/ConclusionsFigs/PDF/}{Conclusions/ConclusionsFigs/}}
\else
    \graphicspath{{Conclusions/ConclusionsFigs/EPS/}{Conclusions/ConclusionsFigs/}}
\fi

\def\baselinestretch{1.66}

In this project we have implemented a sign classifier able to distinguish between signs performed in front of the Kinect sensor. To do this we implemented classes which extend Hidden Markov Models to accept continuous input streams and translate them via k-means clustering to a sequence in a discrete observation space. We then combined left-right topology HMMs in a novel way to form \verb|signModel| objects which use information about all of the joints the Kinect sensor tracks to determine the likelihood that a given sign was performed. Finally we trained these models on a training set and used a hold-out data set to determine optimal parameter values for the acceptance threshold value and number of clusters in the quantization algorithm.

When tested on a test set of 10 instances of each of the 20 signs we found the classifier had an accuracy of 81.6\% or 92.3\% with a grammar restriction applied. These values are not drastically lower than the recognition accuracy when tested on the hold-out set or training set and suggest that the classifier has not suffered too much from overfitting. These recognition rates are quite impressive as our classifier differentiates between actual signs of British Sign Language without ever considering the hands and are better than we had expected to achieve when we began the project.

Whilst certainly of some use, the classifier implemented in this project is not anywhere near a full translation system for BSL as it does not account for the fingers, hand shape of non-manual features of the language. As we have seen this greatly restricts the expressiveness of our system as we require information about the fingers to distinguish certain signs or information about the face to pose questions. Further we have seen that as the number of signs in the dictionary increases the misclassification rate also increases and it follows that the methods used to build our classifier will perform considerably worse when expected to recognise signs from a full lexicon of thousands of words in BSL even if a strict grammar is enforced.

Our classifier's overall accuracy was lower than that of \citet{yamato1992recognizing} which had an average accuracy of 96.0\% for six full body gestures. As our classifier was expected distinguish between more gestures it is no surprise that it has overall accuracy lower than this. However when restricted to a set of only six signs our classifier can have an accuracy between 94\% and 100\%, depending on the choice of signs, which suggests our model is at least as effective as that of Yamato.

\citet{starner1995real} used continuous distribution Hidden Markov Models to build a sign classifier which achieved an accuracy of 90.7\% without a grammar restriction and 97.0\% with a grammar restriction on a test set of 496 sentences drawn from a vocabulary of 40 words of American Sign Language. However their classifier also made use of the shape of the hands during the sign and if such information were available to the our classifier the accuracy would certainly be improved. Nevertheless their results suggest that the method of continuous distribution Hidden Markov Models is worth considering.

\section{Further Work}
\subsection{Improvements to the Current Classifier}
An immediate improvement that could be made to the sign recognition system is to gather training data from a user fluent in British Sign Language. This would likely improve the recognition accuracy of our system as the training sets would be more consistent. As we have implemented an efficient way to gather training sets through the Kinect Sensor this could be achieved fairly easily for a small training set, however it would still be prohibitively time consuming for a single user to generate the training sets for a large dictionary of signs. As the Kinect Sensor is widely available this problem might be solved by crowd-sourcing the training database and allowing users of the system to submit their own training data for new signs.

In building our system we decided to use Hidden Markov Models with discrete observation space over those with continuous observation space. It could be argued that continuous density HMMs might be more suitable for the task of gesture recognition as each individual rendition of a sign can be considered as being normally distributed about some ``true'' path which defines the actual sign. As such we may be able to model the sign much more effectively by using a continuous density Hidden Markov Model in which the probability distribution is some mixture of Gaussians. Failing this we might attempt to implement a more effective discretisation of continuous space. At present we are using Lloyd's algorithm to cluster the input space and this can create small or empty clusters if the cluster number is too high. We could see significant improvements if we implemented a clustering algorithm which ensures no empty clusters and attempts to make clusters evenly sized where possible.

\subsection{Extensions of SignAlign}
An extension of the system to track hands and factor their positions into predicting signs would likely yield significant improvements to sign recognition. For example the signs for ``you'' and ``your'' which are essentially the same if the hand is ignored and are differentiated by a pointing finger or the hand forming a fist cannot be accurately differentiated by the current system. At present the Kinect SDK does not provide information about the locations of the fingers or shapes of the hands and so the system cannot be extended to use hand shapes without using third party software or implementing our own finger tracking; however it is expected that the Kinect will soon be updated to provide hand tracking and so we may be able to use hand tracking in the near future using only the Kinect SDK.

If hand tracking is implemented then we will be able to significantly extend the functionality of our sign recognition system to detect finger spelling. Finger spelling is commonly used in sign language to communicate words with no sign equivalent and is a key part of communicating in sign language. As such the ability to accurately finger spell is integral to a full sign language translation system could be used to overcome the problem of not having training data for a large dictionary. The finger spelling problem consists of distinguishing between 26 distinct letter signs and could be solved using a similar Hidden Markov Model technique to the one presented in this project: By replacing joint tracking with finger and knuckle tracking. As we have obtained a good accuracy in classifying a small dictionary of signs it suggests that this method might produce good results in classifying the different letter signs of finger spelling.

Another feature of sign language which we have ignore in this project is facial expressions. Facial expressions are often used to modify the signs being communicated by the hands. For example the sign ``you'' can be converted to ``are you ...?'' by raising the eyebrows. The Kinect SDK has been used to track facial expressions~\citep{Microsoft:2013:Face} and this work could be integrated with the sign recognition to produce a more robust sign recognition system.


%%% ----------------------------------------------------------------------

% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
